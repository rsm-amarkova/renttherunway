{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4840c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Imports & utilities                            #\n",
    "##################################################\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58b640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 192544\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Load dataset                                   #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def read_rtr(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "DATA_PATH = \"renttherunway_final_data.json\"\n",
    "\n",
    "data = list(read_rtr(DATA_PATH))\n",
    "print(\"Total records:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82129700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 154035\n",
      "Valid size: 38509\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Train / validation split                       #\n",
    "##################################################\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx = set(indices[:split])\n",
    "valid_idx = set(indices[split:])\n",
    "\n",
    "train_data = [data[i] for i in train_idx]\n",
    "valid_data = [data[i] for i in valid_idx]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Valid size:\", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3fa18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratings: 153973\n",
      "Valid ratings: 38489\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 1. Rating prediction                           #\n",
    "##################################################\n",
    "# r ≈ alpha + beta_u + beta_i  (user/item bias model)\n",
    "\n",
    "\n",
    "# Build rating triples (u, i, r) from data\n",
    "def extract_ratings(dataset):\n",
    "    ratings = []\n",
    "    for d in dataset:\n",
    "        if \"rating\" in d and d[\"rating\"] not in (None, \"\", \"nan\"):\n",
    "            try:\n",
    "                r = float(d[\"rating\"])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            u = d[\"user_id\"]\n",
    "            i = d[\"item_id\"]\n",
    "            ratings.append((u, i, r))\n",
    "    return ratings\n",
    "\n",
    "\n",
    "ratingsTrain = extract_ratings(train_data)\n",
    "ratingsValid = extract_ratings(valid_data)\n",
    "\n",
    "print(\"Train ratings:\", len(ratingsTrain))\n",
    "print(\"Valid ratings:\", len(ratingsValid))\n",
    "\n",
    "# Build per-user / per-item maps\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u, b, r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b, r))\n",
    "    ratingsPerItem[b].append((u, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea0a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGlobalAverage(trainRatings):\n",
    "    return sum(r for (_, _, r) in trainRatings) / len(trainRatings)\n",
    "\n",
    "\n",
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    newAlpha = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        newAlpha += r - (betaU.get(u, 0.0) + betaI.get(b, 0.0))\n",
    "    return newAlpha / len(ratingsTrain)\n",
    "\n",
    "\n",
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        num = 0.0\n",
    "        for b, r in ratingsPerUser[u]:\n",
    "            num += r - (alpha + betaI.get(b, 0.0))\n",
    "        newBetaU[u] = num / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU\n",
    "\n",
    "\n",
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    newBetaI = {}\n",
    "    for b in ratingsPerItem:\n",
    "        num = 0.0\n",
    "        for u, r in ratingsPerItem[b]:\n",
    "            num += r - (alpha + betaU.get(u, 0.0))\n",
    "        newBetaI[b] = num / (lamb + len(ratingsPerItem[b]))\n",
    "    return newBetaI\n",
    "\n",
    "\n",
    "def msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsTrain)\n",
    "    reg = 0.0\n",
    "    for u in betaU:\n",
    "        reg += betaU[u] ** 2\n",
    "    for b in betaI:\n",
    "        reg += betaI[b] ** 2\n",
    "    return mse, mse + lamb * reg\n",
    "\n",
    "\n",
    "def validMSE(ratingsValid, alpha, betaU, betaI):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsValid:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsValid)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def train_bias_model(ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=1.0, iters=5):\n",
    "    alpha = getGlobalAverage(ratingsTrain)\n",
    "    betaU = defaultdict(float)\n",
    "    betaI = defaultdict(float)\n",
    "    for _ in range(iters):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "        betaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        betaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating prediction:\n",
      "  Train MSE: 0.9031465439185892\n",
      "  Valid MSE: 2.058030190402456\n"
     ]
    }
   ],
   "source": [
    "alpha, betaU, betaI = train_bias_model(ratingsTrain, ratingsPerUser, ratingsPerItem)\n",
    "\n",
    "train_mse, train_obj = msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb=1.0)\n",
    "valid_mse = validMSE(ratingsValid, alpha, betaU, betaI)\n",
    "\n",
    "print(\"Rating prediction:\")\n",
    "print(\"  Train MSE:\", train_mse)\n",
    "print(\"  Valid MSE:\", valid_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50ee942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITS DEFINITELY OPTIONAL\n",
    "\n",
    "\n",
    "def writePredictionsRating(alpha, betaU, betaI, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd79957",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Rental prediction (Read → Rent)             #\n",
    "##################################################\n",
    "\n",
    "# Treat every observed (user_id, item_id) as a positive \"rented\" interaction.\n",
    "\n",
    "all_rentals = set()\n",
    "userSet = set()\n",
    "itemSet = set()\n",
    "\n",
    "for d in data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    userSet.add(u)\n",
    "    itemSet.add(b)\n",
    "    all_rentals.add((u, b))\n",
    "\n",
    "userList = sorted(list(userSet))\n",
    "itemList = sorted(list(itemSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8570e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rental prediction validation:\n",
      "  Positives: 38497\n",
      "  Negatives: 38497\n"
     ]
    }
   ],
   "source": [
    "# Build positives from validation ratings-style data or from valid_data directly.\n",
    "# We'll just use valid_data as the source of positive rentals.\n",
    "readValid = set()\n",
    "for d in valid_data:\n",
    "    readValid.add((d[\"user_id\"], d[\"item_id\"]))\n",
    "\n",
    "# Generate negative samples: one not-rented item per positive pair\n",
    "notRead = set()\n",
    "for u, b in readValid:\n",
    "    b_neg = random.choice(itemList)\n",
    "    while (u, b_neg) in all_rentals or (u, b_neg) in notRead:\n",
    "        b_neg = random.choice(itemList)\n",
    "    notRead.add((u, b_neg))\n",
    "\n",
    "print(\"Rental prediction validation:\")\n",
    "print(\"  Positives:\", len(readValid))\n",
    "print(\"  Negatives:\", len(notRead))\n",
    "\n",
    "# Popularity counts (on train_data)\n",
    "itemCount = defaultdict(int)\n",
    "for d in train_data:\n",
    "    itemCount[d[\"item_id\"]] += 1\n",
    "\n",
    "totalRead = sum(itemCount.values())\n",
    "mostPopular = sorted([(c, b) for b, c in itemCount.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1620cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseLineStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        if count > totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def improvedStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        # slightly more aggressive threshold\n",
    "        if count > 1.5 * totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def evaluateStrategy(returnSet, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = 1 if b in returnSet else 0\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Baseline popularity accuracy: 0.7070940592773463\n",
      "  Improved popularity accuracy: 0.7417590981115412\n"
     ]
    }
   ],
   "source": [
    "# Baseline popularity model\n",
    "baselineSet = baseLineStrategy(mostPopular, totalRead)\n",
    "baselineAcc = evaluateStrategy(baselineSet, readValid, notRead)\n",
    "print(\"  Baseline popularity accuracy:\", baselineAcc)\n",
    "\n",
    "improvedSet = improvedStrategy(mostPopular, totalRead)\n",
    "improvedAcc = evaluateStrategy(improvedSet, readValid, notRead)\n",
    "print(\"  Improved popularity accuracy:\", improvedAcc)\n",
    "\n",
    "# Jaccard-based strategy (user-based \"similar items\" using co-rentals)\n",
    "\n",
    "ratingsPerUser_all = defaultdict(list)\n",
    "ratingsPerItem_all = defaultdict(list)\n",
    "for d in train_data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    # store dummy rating 1 for structure compatibility\n",
    "    ratingsPerUser_all[u].append((b, 1))\n",
    "    ratingsPerItem_all[b].append((u, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d1c06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Jaccard-based accuracy: 0.7467205236771697\n"
     ]
    }
   ],
   "source": [
    "def jaccardThresh(u, b, ratingsPerItem, ratingsPerUser):\n",
    "    if b not in ratingsPerItem or u not in ratingsPerUser:\n",
    "        # fallback to popularity threshold\n",
    "        return 1 if len(ratingsPerItem.get(b, [])) > 40 else 0\n",
    "    target_users = set([x[0] for x in ratingsPerItem[b]])\n",
    "    maxSim = 0.0\n",
    "    for b2, _ in ratingsPerUser[u]:\n",
    "        users_b2 = set([x[0] for x in ratingsPerItem[b2]])\n",
    "        sim = Jaccard(target_users, users_b2)\n",
    "        if sim > maxSim:\n",
    "            maxSim = sim\n",
    "    if maxSim > 0.013 or len(ratingsPerItem[b]) > 40:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def evaluateJaccard(ratingsPerItem, ratingsPerUser, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))\n",
    "\n",
    "\n",
    "jaccardAcc = evaluateJaccard(ratingsPerItem_all, ratingsPerUser_all, readValid, notRead)\n",
    "print(\"  Jaccard-based accuracy:\", jaccardAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4836754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRent(ratingsPerItem, ratingsPerUser, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defe1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 3. Category prediction (text → category)       #\n",
    "##################################################\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    return \"\".join(c for c in s if c not in punctuation)\n",
    "\n",
    "\n",
    "def build_vocabulary(dataset, field=\"review_text\", vocab_size=1000):\n",
    "    wordCount = defaultdict(int)\n",
    "    for d in dataset:\n",
    "        text = clean_text(d.get(field, \"\"))\n",
    "        for w in text.split():\n",
    "            wordCount[w] += 1\n",
    "    counts = sorted([(c, w) for w, c in wordCount.items()], reverse=True)\n",
    "    words = [w for (c, w) in counts[:vocab_size]]\n",
    "    wordId = {w: i for i, w in enumerate(words)}\n",
    "    wordSet = set(words)\n",
    "    return words, wordId, wordSet\n",
    "\n",
    "\n",
    "def text_features(datum, words, wordId, wordSet, field=\"review_text\"):\n",
    "    feat = [0] * len(words)\n",
    "    text = clean_text(datum.get(field, \"\"))\n",
    "    for w in text.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1)  # bias term\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b026fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category prediction:\n",
      "  Train samples: 154035\n",
      "  Valid samples: 38509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/base-uv/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/opt/base-uv/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train accuracy: 0.6721459408575973\n",
      "  Valid accuracy: 0.6426289958191591\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary on training data\n",
    "cat_words, cat_wordId, cat_wordSet = build_vocabulary(\n",
    "    train_data, field=\"review_text\", vocab_size=1000\n",
    ")\n",
    "\n",
    "\n",
    "# Build X, y for category prediction\n",
    "def build_category_data(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in dataset:\n",
    "        if \"category\" not in d or d[\"category\"] in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        X.append(\n",
    "            text_features(d, cat_words, cat_wordId, cat_wordSet, field=\"review_text\")\n",
    "        )\n",
    "        y.append(d[\"category\"])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_cat_train, y_cat_train = build_category_data(train_data)\n",
    "X_cat_valid, y_cat_valid = build_category_data(valid_data)\n",
    "\n",
    "print(\"Category prediction:\")\n",
    "print(\"  Train samples:\", X_cat_train.shape[0])\n",
    "print(\"  Valid samples:\", X_cat_valid.shape[0])\n",
    "\n",
    "cat_le = LabelEncoder()\n",
    "\n",
    "# Fit on all labels (train + valid)\n",
    "all_cats = np.concatenate([y_cat_train, y_cat_valid])\n",
    "cat_le.fit(all_cats)\n",
    "\n",
    "# Now transform each split\n",
    "y_cat_train_enc = cat_le.transform(y_cat_train)\n",
    "y_cat_valid_enc = cat_le.transform(y_cat_valid)\n",
    "\n",
    "cat_clf = LogisticRegression(max_iter=1000, multi_class=\"auto\")\n",
    "cat_clf.fit(X_cat_train, y_cat_train_enc)\n",
    "\n",
    "cat_train_acc = (cat_clf.predict(X_cat_train) == y_cat_train_enc).mean()\n",
    "cat_valid_acc = (cat_clf.predict(X_cat_valid) == y_cat_valid_enc).mean()\n",
    "\n",
    "print(\"  Train accuracy:\", cat_train_acc)\n",
    "print(\"  Valid accuracy:\", cat_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac6ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsCategory(\n",
    "    model, words, wordId, wordSet, label_encoder, in_pairs_path, out_path\n",
    "):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        pos = 0\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            # You would need to look up the review_text for (u, b) here.\n",
    "            # Placeholder: empty text → all zeros except bias.\n",
    "            feat = [0] * len(words) + [1]\n",
    "            pred_label = model.predict(np.array(feat).reshape(1, -1))[0]\n",
    "            pred_cat = label_encoder.inverse_transform([pred_label])[0]\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred_cat) + \"\\n\")\n",
    "            pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4883ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit prediction:\n",
      "  Train samples: 154035\n",
      "  Valid samples: 38509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/base-uv/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train accuracy: 0.8058687960528451\n",
      "  Valid accuracy: 0.8045651665844348\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 4. Fit prediction (bonus)                      #\n",
    "##################################################\n",
    "# Predict datum[\"fit\"] from review_text (same features as category).\n",
    "\n",
    "\n",
    "def build_fit_data(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in dataset:\n",
    "        if \"fit\" not in d or d[\"fit\"] in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        X.append(\n",
    "            text_features(d, cat_words, cat_wordId, cat_wordSet, field=\"review_text\")\n",
    "        )\n",
    "        y.append(d[\"fit\"])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_fit_train, y_fit_train = build_fit_data(train_data)\n",
    "X_fit_valid, y_fit_valid = build_fit_data(valid_data)\n",
    "\n",
    "print(\"Fit prediction:\")\n",
    "print(\"  Train samples:\", X_fit_train.shape[0])\n",
    "print(\"  Valid samples:\", X_fit_valid.shape[0])\n",
    "\n",
    "fit_le = LabelEncoder()\n",
    "y_fit_train_enc = fit_le.fit_transform(y_fit_train)\n",
    "y_fit_valid_enc = fit_le.transform(y_fit_valid)\n",
    "\n",
    "fit_clf = LogisticRegression(max_iter=1000, multi_class=\"auto\")\n",
    "fit_clf.fit(X_fit_train, y_fit_train_enc)\n",
    "\n",
    "fit_train_acc = (fit_clf.predict(X_fit_train) == y_fit_train_enc).mean()\n",
    "fit_valid_acc = (fit_clf.predict(X_fit_valid) == y_fit_valid_enc).mean()\n",
    "\n",
    "print(\"  Train accuracy:\", fit_train_acc)\n",
    "print(\"  Valid accuracy:\", fit_valid_acc)\n",
    "\n",
    "\n",
    "def writePredictionsFit(\n",
    "    model, words, wordId, wordSet, label_encoder, in_pairs_path, out_path\n",
    "):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            # Again, you'd need (u,b) → review_text mapping here.\n",
    "            feat = [0] * len(words) + [1]\n",
    "            pred_label = model.predict(np.array(feat).reshape(1, -1))[0]\n",
    "            pred_fit = label_encoder.inverse_transform([pred_label])[0]\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred_fit) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-uv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
