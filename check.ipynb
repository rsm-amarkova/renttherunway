{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4840c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Imports & utilities                            #\n",
    "##################################################\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b58b640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 192544\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Load dataset                                   #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def read_rtr(path):\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "DATA_PATH = \"renttherunway_final_data.json.gz\"\n",
    "\n",
    "data = list(read_rtr(DATA_PATH))\n",
    "print(\"Total records:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82129700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 154035\n",
      "Valid size: 38509\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Train / validation split                       #\n",
    "##################################################\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx = set(indices[:split])\n",
    "valid_idx = set(indices[split:])\n",
    "\n",
    "train_data = [data[i] for i in train_idx]\n",
    "valid_data = [data[i] for i in valid_idx]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Valid size:\", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e9ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb1cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469139e",
   "metadata": {},
   "source": [
    "Rating Prediction  \n",
    "* we must regularize a (destabilizes β updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b3fa18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratings: 153971\n",
      "Valid ratings: 38491\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# 1. Rating prediction  #\n",
    "#########################\n",
    "# r ≈ alpha + beta_u + beta_i  (user/item bias model)\n",
    "\n",
    "\n",
    "# extract ratings from dataset and build rating triples (u, i, r) from data\n",
    "def extract_ratings(dataset):\n",
    "    ratings = []\n",
    "    for d in dataset:\n",
    "        if \"rating\" in d and d[\"rating\"] not in (None, \"\", \"nan\"):\n",
    "            try:\n",
    "                r = float(d[\"rating\"])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            u = d[\"user_id\"]\n",
    "            i = d[\"item_id\"]\n",
    "            ratings.append((u, i, r))\n",
    "    return ratings\n",
    "\n",
    "\n",
    "ratingsTrain = extract_ratings(train_data)\n",
    "ratingsValid = extract_ratings(valid_data)\n",
    "\n",
    "print(\"Train ratings:\", len(ratingsTrain))\n",
    "print(\"Valid ratings:\", len(ratingsValid))\n",
    "\n",
    "# Build per-user / per-item maps\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u, b, r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b, r))\n",
    "    ratingsPerItem[b].append((u, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ea0a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the global average rating\n",
    "def getGlobalAverage(trainRatings):\n",
    "    return sum(r for (_, _, r) in trainRatings) / len(trainRatings)\n",
    "\n",
    "\n",
    "######################\n",
    "# Bias Model Updates #\n",
    "######################\n",
    "\n",
    "\n",
    "# improving alpha update with regularization\n",
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    newAlpha = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        newAlpha += r - (betaU.get(u, 0.0) + betaI.get(b, 0.0))\n",
    "    return newAlpha / (len(ratingsTrain) + lamb)\n",
    "\n",
    "\n",
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        num = 0.0\n",
    "        for b, r in ratingsPerUser[u]:\n",
    "            num += r - (alpha + betaI.get(b, 0.0))\n",
    "        newBetaU[u] = num / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU\n",
    "\n",
    "\n",
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    newBetaI = {}\n",
    "    for b in ratingsPerItem:\n",
    "        num = 0.0\n",
    "        for u, r in ratingsPerItem[b]:\n",
    "            num += r - (alpha + betaU.get(u, 0.0))\n",
    "        newBetaI[b] = num / (lamb + len(ratingsPerItem[b]))\n",
    "    return newBetaI\n",
    "\n",
    "\n",
    "###################\n",
    "# MSE COMPUTATION #\n",
    "###################\n",
    "\n",
    "\n",
    "def msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsTrain)\n",
    "\n",
    "    # regularization penalty\n",
    "    reg = sum(b**2 for b in betaU.values()) + sum(b**2 for b in betaI.values())\n",
    "    return mse, mse + lamb * reg\n",
    "\n",
    "\n",
    "def validMSE(ratingsValid, alpha, betaU, betaI):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsValid:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsValid)\n",
    "    return mse\n",
    "\n",
    "\n",
    "##################\n",
    "# TRAINING LOOP  #\n",
    "##################\n",
    "\n",
    "\n",
    "def train_bias_model(ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=1.0, iters=30):\n",
    "    alpha = getGlobalAverage(ratingsTrain)\n",
    "    betaU = defaultdict(float)\n",
    "    betaI = defaultdict(float)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "\n",
    "        newBetaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        newBetaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "\n",
    "        betaU.update(newBetaU)  # refine existing estimates, not reset\n",
    "        betaI.update(newBetaI)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f58498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0.1  Valid MSE=2.3675\n",
      "λ=0.3  Valid MSE=2.2266\n",
      "λ=1  Valid MSE=2.0357\n",
      "λ=3  Valid MSE=1.9229\n",
      "λ=10  Valid MSE=1.8926\n",
      "Rating prediction:\n",
      "  Train MSE: 1.567066834286779\n",
      "  Valid MSE: 1.8925909950809328\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best lambda\n",
    "# tuning lambda:\n",
    "for lamb in [0.1, 0.3, 1, 3, 10]:\n",
    "    alpha_tmp, bu_tmp, bi_tmp = train_bias_model(\n",
    "        ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=lamb, iters=30\n",
    "    )\n",
    "    print(\n",
    "        f\"λ={lamb}  Valid MSE={validMSE(ratingsValid, alpha_tmp, bu_tmp, bi_tmp):.4f}\"\n",
    "    )\n",
    "\n",
    "# --- Pick λ manually after seeing results ---\n",
    "best_lambda = 10.0\n",
    "\n",
    "alpha, betaU, betaI = train_bias_model(\n",
    "    ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=best_lambda, iters=30\n",
    ")\n",
    "\n",
    "# final metrics\n",
    "train_mse, train_obj = msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb=best_lambda)\n",
    "valid_mse = validMSE(ratingsValid, alpha, betaU, betaI)\n",
    "\n",
    "print(\"Rating prediction:\")\n",
    "print(\"  Train MSE:\", train_mse)\n",
    "print(\"  Valid MSE:\", valid_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f50ee942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITS DEFINITELY OPTIONAL\n",
    "\n",
    "\n",
    "def writePredictionsRating(alpha, betaU, betaI, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdd79957",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Rental prediction (Read → Rent)             #\n",
    "##################################################\n",
    "\n",
    "# Treat every observed (user_id, item_id) as a positive \"rented\" interaction.\n",
    "\n",
    "all_rentals = set()\n",
    "userSet = set()\n",
    "itemSet = set()\n",
    "\n",
    "for d in data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    userSet.add(u)\n",
    "    itemSet.add(b)\n",
    "    all_rentals.add((u, b))\n",
    "\n",
    "userList = sorted(list(userSet))\n",
    "itemList = sorted(list(itemSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8570e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rental prediction validation:\n",
      "  Positives: 38498\n",
      "  Negatives: 38498\n"
     ]
    }
   ],
   "source": [
    "# Build positives from validation ratings-style data or from valid_data directly.\n",
    "# We'll just use valid_data as the source of positive rentals.\n",
    "readValid = set()\n",
    "for d in valid_data:\n",
    "    readValid.add((d[\"user_id\"], d[\"item_id\"]))\n",
    "\n",
    "# Generate negative samples: one not-rented item per positive pair\n",
    "notRead = set()\n",
    "for u, b in readValid:\n",
    "    b_neg = random.choice(itemList)\n",
    "    while (u, b_neg) in all_rentals or (u, b_neg) in notRead:\n",
    "        b_neg = random.choice(itemList)\n",
    "    notRead.add((u, b_neg))\n",
    "\n",
    "print(\"Rental prediction validation:\")\n",
    "print(\"  Positives:\", len(readValid))\n",
    "print(\"  Negatives:\", len(notRead))\n",
    "\n",
    "# Popularity counts (on train_data)\n",
    "itemCount = defaultdict(int)\n",
    "for d in train_data:\n",
    "    itemCount[d[\"item_id\"]] += 1\n",
    "\n",
    "totalRead = sum(itemCount.values())\n",
    "mostPopular = sorted([(c, b) for b, c in itemCount.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1620cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseLineStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        if count > totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def improvedStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        # slightly more aggressive threshold\n",
    "        if count > 1.5 * totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def evaluateStrategy(returnSet, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = 1 if b in returnSet else 0\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c48e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Baseline popularity accuracy: 0.7063224063587719\n",
      "  Improved popularity accuracy: 0.743869811418775\n"
     ]
    }
   ],
   "source": [
    "# Baseline popularity model\n",
    "baselineSet = baseLineStrategy(mostPopular, totalRead)\n",
    "baselineAcc = evaluateStrategy(baselineSet, readValid, notRead)\n",
    "print(\"  Baseline popularity accuracy:\", baselineAcc)\n",
    "\n",
    "improvedSet = improvedStrategy(mostPopular, totalRead)\n",
    "improvedAcc = evaluateStrategy(improvedSet, readValid, notRead)\n",
    "print(\"  Improved popularity accuracy:\", improvedAcc)\n",
    "\n",
    "# Jaccard-based strategy (user-based \"similar items\" using co-rentals)\n",
    "\n",
    "ratingsPerUser_all = defaultdict(list)\n",
    "ratingsPerItem_all = defaultdict(list)\n",
    "for d in train_data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    # store dummy rating 1 for structure compatibility\n",
    "    ratingsPerUser_all[u].append((b, 1))\n",
    "    ratingsPerItem_all[b].append((u, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d1c06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Jaccard-based accuracy: 0.7464673489531923\n"
     ]
    }
   ],
   "source": [
    "def jaccardThresh(u, b, ratingsPerItem, ratingsPerUser):\n",
    "    if b not in ratingsPerItem or u not in ratingsPerUser:\n",
    "        # fallback to popularity threshold\n",
    "        return 1 if len(ratingsPerItem.get(b, [])) > 40 else 0\n",
    "    target_users = set([x[0] for x in ratingsPerItem[b]])\n",
    "    maxSim = 0.0\n",
    "    for b2, _ in ratingsPerUser[u]:\n",
    "        users_b2 = set([x[0] for x in ratingsPerItem[b2]])\n",
    "        sim = Jaccard(target_users, users_b2)\n",
    "        if sim > maxSim:\n",
    "            maxSim = sim\n",
    "    if maxSim > 0.013 or len(ratingsPerItem[b]) > 40:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def evaluateJaccard(ratingsPerItem, ratingsPerUser, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))\n",
    "\n",
    "\n",
    "jaccardAcc = evaluateJaccard(ratingsPerItem_all, ratingsPerUser_all, readValid, notRead)\n",
    "print(\"  Jaccard-based accuracy:\", jaccardAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4836754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRent(ratingsPerItem, ratingsPerUser, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rented-for prediction:\n",
      "  Total labeled train examples: 154026\n",
      "  Label counts (top 10): [('wedding', 46300), ('formal affair', 32245), ('party', 28364), ('everyday', 13459), ('other', 12353), ('work', 12110), ('date', 5919), ('vacation', 3275), ('party: cocktail', 1)]\n",
      "  Top 10 labels: ['wedding', 'formal affair', 'party', 'everyday', 'other', 'work', 'date', 'vacation', 'party: cocktail']\n",
      "  Train samples (top-k only): 154026\n",
      "  Valid samples (top-k only): 38508\n",
      "  Unique rented-for labels (train): 9\n",
      "  X_rf_train shape: (154026, 12000)\n",
      "\n",
      "Tuning C:\n",
      "  C=0.5   Train=0.6507  Valid=0.5985\n",
      "  C=1.0   Train=0.6673  Valid=0.5960\n",
      "  C=2.0   Train=0.6838  Valid=0.5904\n",
      "  C=3.0   Train=0.6920  Valid=0.5864\n",
      "  C=5.0   Train=0.7016  Valid=0.5798\n",
      "\n",
      "Best C: 0.5  (valid accuracy = 0.5985)\n",
      "\n",
      "Final model (rented-for prediction):\n",
      "  Train accuracy: 0.6506628750990092\n",
      "  Valid accuracy: 0.5984730445621689\n",
      "\n",
      "Classification report (valid):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         date      0.665     0.247     0.360      1469\n",
      "     everyday      0.555     0.633     0.591      3363\n",
      "formal affair      0.597     0.615     0.606      8163\n",
      "        other      0.533     0.259     0.349      3035\n",
      "        party      0.542     0.540     0.541      7262\n",
      "     vacation      0.660     0.199     0.305       800\n",
      "      wedding      0.639     0.796     0.709     11484\n",
      "         work      0.620     0.520     0.566      2932\n",
      "\n",
      "     accuracy                          0.598     38508\n",
      "    macro avg      0.601     0.476     0.503     38508\n",
      " weighted avg      0.596     0.598     0.584     38508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Task 2 – Category / Event Prediction       #\n",
    "#            (text → rented for)                #\n",
    "##################################################\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# IMPORTANT: use the actual key in the data\n",
    "label_field = \"rented for\"  # key in the JSON\n",
    "top_k = 10  # keep only the top-k most common labels\n",
    "\n",
    "\n",
    "def collect_labels(dataset):\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        lab = d.get(label_field)\n",
    "        if lab in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        labels.append(lab)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# --- 1. Find top-k most common \"rented for\" labels on train set ---\n",
    "all_train_labels = collect_labels(train_data)\n",
    "label_counts = Counter(all_train_labels)\n",
    "\n",
    "print(\"Rented-for prediction:\")\n",
    "print(\"  Total labeled train examples:\", len(all_train_labels))\n",
    "print(\"  Label counts (top 10):\", label_counts.most_common(10))\n",
    "\n",
    "top_k_labels = [lab for lab, _ in label_counts.most_common(top_k)]\n",
    "print(\"  Top\", top_k, \"labels:\", top_k_labels)\n",
    "\n",
    "\n",
    "def extract_texts_and_rented_for(dataset, allowed_labels):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        lab = d.get(label_field)\n",
    "        if lab in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        if lab not in allowed_labels:  # drop rare labels\n",
    "            continue\n",
    "        texts.append(get_full_text(d))  # reuse review_text + summary\n",
    "        labels.append(lab)\n",
    "    return texts, np.array(labels)\n",
    "\n",
    "\n",
    "# --- 2. Build train/valid sets restricted to top-k labels ---\n",
    "\n",
    "train_texts_rf, y_rf_train = extract_texts_and_rented_for(train_data, top_k_labels)\n",
    "valid_texts_rf, y_rf_valid = extract_texts_and_rented_for(valid_data, top_k_labels)\n",
    "\n",
    "print(\"  Train samples (top-k only):\", len(train_texts_rf))\n",
    "print(\"  Valid samples (top-k only):\", len(valid_texts_rf))\n",
    "\n",
    "if len(train_texts_rf) == 0:\n",
    "    raise ValueError(\n",
    "        \"No training samples found for 'rented for'. \"\n",
    "        \"Check that label_field matches the key in your data.\"\n",
    "    )\n",
    "\n",
    "print(\"  Unique rented-for labels (train):\", len(np.unique(y_rf_train)))\n",
    "\n",
    "# --- 3. TF-IDF features (separate vectorizer for this task) ---\n",
    "\n",
    "rf_vectorizer = TfidfVectorizer(\n",
    "    max_features=12000,  # a bit richer than 8000\n",
    "    ngram_range=(1, 2),  # unigrams + bigrams\n",
    "    min_df=3,  # drop extremely rare tokens\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "X_rf_train = rf_vectorizer.fit_transform(train_texts_rf)\n",
    "X_rf_valid = rf_vectorizer.transform(valid_texts_rf)\n",
    "\n",
    "print(\"  X_rf_train shape:\", X_rf_train.shape)\n",
    "\n",
    "# --- 4. Encode rented-for labels ---\n",
    "\n",
    "rf_le = LabelEncoder()\n",
    "all_rf_labels = np.concatenate([y_rf_train, y_rf_valid])\n",
    "rf_le.fit(all_rf_labels)\n",
    "\n",
    "y_rf_train_enc = rf_le.transform(y_rf_train)\n",
    "y_rf_valid_enc = rf_le.transform(y_rf_valid)\n",
    "\n",
    "# --- 5. Hyperparameter search for best C ---\n",
    "\n",
    "Cs = [0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "best_C = None\n",
    "best_valid_acc = -1.0\n",
    "best_clf = None\n",
    "\n",
    "print(\"\\nTuning C:\")\n",
    "for C in Cs:\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=300,\n",
    "        solver=\"saga\",\n",
    "        n_jobs=-1,\n",
    "        C=C,\n",
    "        random_state=0,\n",
    "    )\n",
    "    clf.fit(X_rf_train, y_rf_train_enc)\n",
    "    train_pred = clf.predict(X_rf_train)\n",
    "    valid_pred = clf.predict(X_rf_valid)\n",
    "\n",
    "    train_acc = (train_pred == y_rf_train_enc).mean()\n",
    "    valid_acc = (valid_pred == y_rf_valid_enc).mean()\n",
    "\n",
    "    print(f\"  C={C:<4}  Train={train_acc:.4f}  Valid={valid_acc:.4f}\")\n",
    "\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_C = C\n",
    "        best_clf = clf\n",
    "\n",
    "print(f\"\\nBest C: {best_C}  (valid accuracy = {best_valid_acc:.4f})\")\n",
    "\n",
    "# --- 6. Evaluate best model: accuracy + per-class metrics ---\n",
    "\n",
    "rf_train_pred = best_clf.predict(X_rf_train)\n",
    "rf_valid_pred = best_clf.predict(X_rf_valid)\n",
    "\n",
    "rf_train_acc = (rf_train_pred == y_rf_train_enc).mean()\n",
    "rf_valid_acc = (rf_valid_pred == y_rf_valid_enc).mean()\n",
    "\n",
    "print(\"\\nFinal model (rented-for prediction):\")\n",
    "print(\"  Train accuracy:\", rf_train_acc)\n",
    "print(\"  Valid accuracy:\", rf_valid_acc)\n",
    "\n",
    "# Decode predictions back to label names for nice reports\n",
    "y_rf_valid_pred_labels = rf_le.inverse_transform(rf_valid_pred)\n",
    "\n",
    "print(\"\\nClassification report (valid):\")\n",
    "print(classification_report(y_rf_valid, y_rf_valid_pred_labels, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac6ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsCategory(\n",
    "    model, words, wordId, wordSet, label_encoder, in_pairs_path, out_path\n",
    "):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        pos = 0\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            # You would need to look up the review_text for (u, b) here.\n",
    "            # Placeholder: empty text → all zeros except bias.\n",
    "            feat = [0] * len(words) + [1]\n",
    "            pred_label = model.predict(np.array(feat).reshape(1, -1))[0]\n",
    "            pred_cat = label_encoder.inverse_transform([pred_label])[0]\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred_cat) + \"\\n\")\n",
    "            pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e333398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_for_category(d):\n",
    "    \"\"\"\n",
    "    Build the text used for both category and fit models.\n",
    "    Uses review_text and (optionally) review_summary if it exists.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    txt = d.get(\"review_text\")\n",
    "    summ = d.get(\"review_summary\")  # it's okay if this field doesn't exist\n",
    "\n",
    "    if txt:\n",
    "        parts.append(clean_text(txt))\n",
    "    if summ:\n",
    "        parts.append(clean_text(summ))\n",
    "\n",
    "    return \" \".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ef741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit prediction (classification):\n",
      "  Train samples: 154035\n",
      "  Valid samples: 154035\n",
      "  Train accuracy: 0.7952088810984517\n",
      "  Valid accuracy: 0.7428912721701421\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 4. Fit prediction (text → fit label)           #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def extract_texts_and_fit_labels(dataset):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        lab = d.get(\"fit\")\n",
    "        # skip missing / bad\n",
    "        if lab in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        texts.append(\n",
    "            get_text_for_category(d)\n",
    "        )  # uses review_text (+ summary if you have it)\n",
    "        labels.append(lab)\n",
    "    return texts, np.array(labels)\n",
    "\n",
    "\n",
    "train_texts_fit, y_fit_train = extract_texts_and_fit_labels(train_data)\n",
    "valid_texts_fit, y_fit_valid = extract_texts_and_fit_labels(valid_data)\n",
    "\n",
    "print(\"Fit prediction (classification):\")\n",
    "print(\"  Train samples:\", len(train_texts_fit))\n",
    "print(\"  Valid samples:\", len(train_texts_fit))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Text features for fit prediction\n",
    "fit_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000, ngram_range=(1, 2), min_df=3, sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_fit_train = fit_vectorizer.fit_transform(train_texts_fit)\n",
    "X_fit_valid = fit_vectorizer.transform(valid_texts_fit)\n",
    "\n",
    "# Encode fit labels (e.g. \"Small\", \"True to Size\", \"Large\")\n",
    "fit_le = LabelEncoder()\n",
    "all_fit_labels = np.concatenate([y_fit_train, y_fit_valid])\n",
    "fit_le.fit(all_fit_labels)\n",
    "\n",
    "\n",
    "y_fit_train_enc = fit_le.transform(y_fit_train)\n",
    "y_fit_valid_enc = fit_le.transform(y_fit_valid)\n",
    "\n",
    "fit_clf = LogisticRegression(\n",
    "    max_iter=200, solver=\"saga\", n_jobs=-1, C=2.0, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "fit_clf.fit(X_fit_train, y_fit_train_enc)\n",
    "\n",
    "fit_train_acc = (fit_clf.predict(X_fit_train) == y_fit_train_enc).mean()\n",
    "fit_valid_acc = (fit_clf.predict(X_fit_valid) == y_fit_valid_enc).mean()\n",
    "\n",
    "print(\"  Train accuracy:\", fit_train_acc)\n",
    "print(\"  Valid accuracy:\", fit_valid_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-uv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
