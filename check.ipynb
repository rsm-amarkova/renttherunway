{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4840c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Imports & utilities                            #\n",
    "##################################################\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58b640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 192544\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Load dataset                                   #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def read_rtr(path):\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "DATA_PATH = \"renttherunway_final_data.json.gz\"\n",
    "\n",
    "data = list(read_rtr(DATA_PATH))\n",
    "print(\"Total records:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82129700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 154035\n",
      "Valid size: 38509\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Train / validation split                       #\n",
    "##################################################\n",
    "\n",
    "indices = list(range(len(data)))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx = set(indices[:split])\n",
    "valid_idx = set(indices[split:])\n",
    "\n",
    "train_data = [data[i] for i in train_idx]\n",
    "valid_data = [data[i] for i in valid_idx]\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Valid size:\", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e9ff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb1cfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Shared helpers                                 #\n",
    "##################################################\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom > 0:\n",
    "        return numer / denom\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469139e",
   "metadata": {},
   "source": [
    "Rating Prediction  \n",
    "* we must regularize a (destabilizes β updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b3fa18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratings: 153967\n",
      "Valid ratings: 38495\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# 1. Rating prediction  #\n",
    "#########################\n",
    "# r ≈ alpha + beta_u + beta_i  (user/item bias model)\n",
    "\n",
    "\n",
    "# extract ratings from dataset and build rating triples (u, i, r) from data\n",
    "def extract_ratings(dataset):\n",
    "    ratings = []\n",
    "    for d in dataset:\n",
    "        if \"rating\" in d and d[\"rating\"] not in (None, \"\", \"nan\"):\n",
    "            try:\n",
    "                r = float(d[\"rating\"])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            u = d[\"user_id\"]\n",
    "            i = d[\"item_id\"]\n",
    "            ratings.append((u, i, r))\n",
    "    return ratings\n",
    "\n",
    "\n",
    "ratingsTrain = extract_ratings(train_data)\n",
    "ratingsValid = extract_ratings(valid_data)\n",
    "\n",
    "print(\"Train ratings:\", len(ratingsTrain))\n",
    "print(\"Valid ratings:\", len(ratingsValid))\n",
    "\n",
    "# Build per-user / per-item maps\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u, b, r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b, r))\n",
    "    ratingsPerItem[b].append((u, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea0a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the global average rating\n",
    "def getGlobalAverage(trainRatings):\n",
    "    return sum(r for (_, _, r) in trainRatings) / len(trainRatings)\n",
    "\n",
    "\n",
    "######################\n",
    "# Bias Model Updates #\n",
    "######################\n",
    "\n",
    "\n",
    "# improving alpha update with regularization\n",
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    newAlpha = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        newAlpha += r - (betaU.get(u, 0.0) + betaI.get(b, 0.0))\n",
    "    return newAlpha / (len(ratingsTrain) + lamb)\n",
    "\n",
    "\n",
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        num = 0.0\n",
    "        for b, r in ratingsPerUser[u]:\n",
    "            num += r - (alpha + betaI.get(b, 0.0))\n",
    "        newBetaU[u] = num / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU\n",
    "\n",
    "\n",
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    newBetaI = {}\n",
    "    for b in ratingsPerItem:\n",
    "        num = 0.0\n",
    "        for u, r in ratingsPerItem[b]:\n",
    "            num += r - (alpha + betaU.get(u, 0.0))\n",
    "        newBetaI[b] = num / (lamb + len(ratingsPerItem[b]))\n",
    "    return newBetaI\n",
    "\n",
    "\n",
    "###################\n",
    "# MSE COMPUTATION #\n",
    "###################\n",
    "\n",
    "\n",
    "def msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsTrain:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsTrain)\n",
    "\n",
    "    # regularization penalty\n",
    "    reg = sum(b**2 for b in betaU.values()) + sum(b**2 for b in betaI.values())\n",
    "    return mse, mse + lamb * reg\n",
    "\n",
    "\n",
    "def validMSE(ratingsValid, alpha, betaU, betaI):\n",
    "    mse = 0.0\n",
    "    for u, b, r in ratingsValid:\n",
    "        pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "        mse += (r - pred) ** 2\n",
    "    mse /= len(ratingsValid)\n",
    "    return mse\n",
    "\n",
    "\n",
    "##################\n",
    "# TRAINING LOOP  #\n",
    "##################\n",
    "\n",
    "\n",
    "def train_bias_model(ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=1.0, iters=30):\n",
    "    alpha = getGlobalAverage(ratingsTrain)\n",
    "    betaU = defaultdict(float)\n",
    "    betaI = defaultdict(float)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "\n",
    "        newBetaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        newBetaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "\n",
    "        betaU.update(newBetaU)  # refine existing estimates, not reset\n",
    "        betaI.update(newBetaI)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f58498df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0.1  Valid MSE=2.4038\n",
      "λ=0.3  Valid MSE=2.2615\n",
      "λ=1  Valid MSE=2.0697\n",
      "λ=3  Valid MSE=1.9580\n",
      "λ=10  Valid MSE=1.9305\n",
      "Rating prediction:\n",
      "  Train MSE: 1.5598729335473964\n",
      "  Valid MSE: 1.9304872954213486\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search for best lambda\n",
    "# tuning lambda:\n",
    "for lamb in [0.1, 0.3, 1, 3, 10]:\n",
    "    alpha_tmp, bu_tmp, bi_tmp = train_bias_model(\n",
    "        ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=lamb, iters=30\n",
    "    )\n",
    "    print(\n",
    "        f\"λ={lamb}  Valid MSE={validMSE(ratingsValid, alpha_tmp, bu_tmp, bi_tmp):.4f}\"\n",
    "    )\n",
    "\n",
    "# --- Pick λ manually after seeing results ---\n",
    "best_lambda = 10.0\n",
    "\n",
    "alpha, betaU, betaI = train_bias_model(\n",
    "    ratingsTrain, ratingsPerUser, ratingsPerItem, lamb=best_lambda, iters=30\n",
    ")\n",
    "\n",
    "# final metrics\n",
    "train_mse, train_obj = msePlusReg(ratingsTrain, alpha, betaU, betaI, lamb=best_lambda)\n",
    "valid_mse = validMSE(ratingsValid, alpha, betaU, betaI)\n",
    "\n",
    "print(\"Rating prediction:\")\n",
    "print(\"  Train MSE:\", train_mse)\n",
    "print(\"  Valid MSE:\", valid_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50ee942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITS DEFINITELY OPTIONAL\n",
    "\n",
    "\n",
    "def writePredictionsRating(alpha, betaU, betaI, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = alpha + betaU.get(u, 0.0) + betaI.get(b, 0.0)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdd79957",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Rental prediction (Read → Rent)             #\n",
    "##################################################\n",
    "\n",
    "# Treat every observed (user_id, item_id) as a positive \"rented\" interaction.\n",
    "\n",
    "all_rentals = set()\n",
    "userSet = set()\n",
    "itemSet = set()\n",
    "\n",
    "for d in data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    userSet.add(u)\n",
    "    itemSet.add(b)\n",
    "    all_rentals.add((u, b))\n",
    "\n",
    "userList = sorted(list(userSet))\n",
    "itemList = sorted(list(itemSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8570e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rental prediction validation:\n",
      "  Positives: 38503\n",
      "  Negatives: 38503\n"
     ]
    }
   ],
   "source": [
    "# Build positives from validation ratings-style data or from valid_data directly.\n",
    "# We'll just use valid_data as the source of positive rentals.\n",
    "readValid = set()\n",
    "for d in valid_data:\n",
    "    readValid.add((d[\"user_id\"], d[\"item_id\"]))\n",
    "\n",
    "# Generate negative samples: one not-rented item per positive pair\n",
    "notRead = set()\n",
    "for u, b in readValid:\n",
    "    b_neg = random.choice(itemList)\n",
    "    while (u, b_neg) in all_rentals or (u, b_neg) in notRead:\n",
    "        b_neg = random.choice(itemList)\n",
    "    notRead.add((u, b_neg))\n",
    "\n",
    "print(\"Rental prediction validation:\")\n",
    "print(\"  Positives:\", len(readValid))\n",
    "print(\"  Negatives:\", len(notRead))\n",
    "\n",
    "# Popularity counts (on train_data)\n",
    "itemCount = defaultdict(int)\n",
    "for d in train_data:\n",
    "    itemCount[d[\"item_id\"]] += 1\n",
    "\n",
    "totalRead = sum(itemCount.values())\n",
    "mostPopular = sorted([(c, b) for b, c in itemCount.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1620cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseLineStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        if count > totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def improvedStrategy(mostPopular, totalRead):\n",
    "    chosen = set()\n",
    "    count = 0\n",
    "    for c, b in mostPopular:\n",
    "        count += c\n",
    "        chosen.add(b)\n",
    "        # slightly more aggressive threshold\n",
    "        if count > 1.5 * totalRead / 2:\n",
    "            break\n",
    "    return chosen\n",
    "\n",
    "\n",
    "def evaluateStrategy(returnSet, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = 1 if b in returnSet else 0\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c48e44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Baseline popularity accuracy: 0.7073994234215516\n",
      "  Improved popularity accuracy: 0.7426044723787757\n"
     ]
    }
   ],
   "source": [
    "# Baseline popularity model\n",
    "baselineSet = baseLineStrategy(mostPopular, totalRead)\n",
    "baselineAcc = evaluateStrategy(baselineSet, readValid, notRead)\n",
    "print(\"  Baseline popularity accuracy:\", baselineAcc)\n",
    "\n",
    "improvedSet = improvedStrategy(mostPopular, totalRead)\n",
    "improvedAcc = evaluateStrategy(improvedSet, readValid, notRead)\n",
    "print(\"  Improved popularity accuracy:\", improvedAcc)\n",
    "\n",
    "# Jaccard-based strategy (user-based \"similar items\" using co-rentals)\n",
    "\n",
    "ratingsPerUser_all = defaultdict(list)\n",
    "ratingsPerItem_all = defaultdict(list)\n",
    "for d in train_data:\n",
    "    u = d[\"user_id\"]\n",
    "    b = d[\"item_id\"]\n",
    "    # store dummy rating 1 for structure compatibility\n",
    "    ratingsPerUser_all[u].append((b, 1))\n",
    "    ratingsPerItem_all[b].append((u, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d1c06a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Jaccard-based accuracy: 0.7465002727060229\n"
     ]
    }
   ],
   "source": [
    "def jaccardThresh(u, b, ratingsPerItem, ratingsPerUser):\n",
    "    if b not in ratingsPerItem or u not in ratingsPerUser:\n",
    "        # fallback to popularity threshold\n",
    "        return 1 if len(ratingsPerItem.get(b, [])) > 40 else 0\n",
    "    target_users = set([x[0] for x in ratingsPerItem[b]])\n",
    "    maxSim = 0.0\n",
    "    for b2, _ in ratingsPerUser[u]:\n",
    "        users_b2 = set([x[0] for x in ratingsPerItem[b2]])\n",
    "        sim = Jaccard(target_users, users_b2)\n",
    "        if sim > maxSim:\n",
    "            maxSim = sim\n",
    "    if maxSim > 0.013 or len(ratingsPerItem[b]) > 40:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def evaluateJaccard(ratingsPerItem, ratingsPerUser, readValid, notRead):\n",
    "    correct = 0\n",
    "    for label, sample in [(1, readValid), (0, notRead)]:\n",
    "        for u, b in sample:\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    return correct / (len(readValid) + len(notRead))\n",
    "\n",
    "\n",
    "jaccardAcc = evaluateJaccard(ratingsPerItem_all, ratingsPerUser_all, readValid, notRead)\n",
    "print(\"  Jaccard-based accuracy:\", jaccardAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4836754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsRent(ratingsPerItem, ratingsPerUser, in_pairs_path, out_path):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            pred = jaccardThresh(u, b, ratingsPerItem, ratingsPerUser)\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "defe1d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category prediction:\n",
      "  Train samples: 154035\n",
      "  Valid samples: 38509\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 3. Category prediction (text → category)       #\n",
    "##################################################\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    return \"\".join(c for c in s if c not in punctuation)\n",
    "\n",
    "\n",
    "def extract_texts_and_labels(dataset, field=\"review_text\"):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for d in dataset:\n",
    "        cat = d.get(\"category\")\n",
    "        if cat in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        texts.append(clean_text(d.get(field, \"\")))\n",
    "        labels.append(cat)\n",
    "    return texts, np.array(labels)\n",
    "\n",
    "\n",
    "# Build text + labels\n",
    "train_texts, y_cat_train = extract_texts_and_labels(train_data)\n",
    "valid_texts, y_cat_valid = extract_texts_and_labels(valid_data)\n",
    "\n",
    "print(\"Category prediction:\")\n",
    "print(\"  Train samples:\", len(train_texts))\n",
    "print(\"  Valid samples:\", len(valid_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b026fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train accuracy: 0.6758658746388808\n",
      "  Valid accuracy: 0.6592225194110468\n"
     ]
    }
   ],
   "source": [
    "# Fast, high-quality text features (sparse TF-IDF)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # increase vocab vs 1000 words → better accuracy\n",
    "    ngram_range=(1, 2),  # unigrams + bigrams\n",
    "    min_df=5,  # ignore very rare words (helps speed & noise)\n",
    ")\n",
    "\n",
    "X_cat_train = vectorizer.fit_transform(train_texts)\n",
    "X_cat_valid = vectorizer.transform(valid_texts)\n",
    "\n",
    "# Encode labels (train + valid so we don't hit unseen-label errors)\n",
    "cat_le = LabelEncoder()\n",
    "all_cats = np.concatenate([y_cat_train, y_cat_valid])\n",
    "cat_le.fit(all_cats)\n",
    "\n",
    "y_cat_train_enc = cat_le.transform(y_cat_train)\n",
    "y_cat_valid_enc = cat_le.transform(y_cat_valid)\n",
    "\n",
    "# Faster solver that supports sparse matrices + multi-class\n",
    "cat_clf = LogisticRegression(\n",
    "    max_iter=200,  # usually enough with good features\n",
    "    solver=\"saga\",  # works well with many features & sparse X\n",
    "    n_jobs=-1,  # use all CPU cores → much faster\n",
    ")\n",
    "\n",
    "cat_clf.fit(X_cat_train, y_cat_train_enc)\n",
    "\n",
    "cat_train_acc = (cat_clf.predict(X_cat_train) == y_cat_train_enc).mean()\n",
    "cat_valid_acc = (cat_clf.predict(X_cat_valid) == y_cat_valid_enc).mean()\n",
    "\n",
    "print(\"  Train accuracy:\", cat_train_acc)\n",
    "print(\"  Valid accuracy:\", cat_valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePredictionsCategory(\n",
    "    model, words, wordId, wordSet, label_encoder, in_pairs_path, out_path\n",
    "):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        pos = 0\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            # You would need to look up the review_text for (u, b) here.\n",
    "            # Placeholder: empty text → all zeros except bias.\n",
    "            feat = [0] * len(words) + [1]\n",
    "            pred_label = model.predict(np.array(feat).reshape(1, -1))[0]\n",
    "            pred_cat = label_encoder.inverse_transform([pred_label])[0]\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred_cat) + \"\\n\")\n",
    "            pos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4883ae79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m         y.append(d[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array(X), np.array(y)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m X_fit_train, y_fit_train = \u001b[43mbuild_fit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m X_fit_valid, y_fit_valid = build_fit_data(valid_data)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFit prediction:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mbuild_fit_data\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m d \u001b[38;5;129;01mor\u001b[39;00m d[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     13\u001b[39m     X.append(\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[43mtext_features\u001b[49m(d, cat_words, cat_wordId, cat_wordSet, field=\u001b[33m\"\u001b[39m\u001b[33mreview_text\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     y.append(d[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(X), np.array(y)\n",
      "\u001b[31mNameError\u001b[39m: name 'text_features' is not defined"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 4. Fit prediction (bonus)                      #\n",
    "##################################################\n",
    "# Predict datum[\"fit\"] from review_text (same features as category).\n",
    "\n",
    "\n",
    "def build_fit_data(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in dataset:\n",
    "        if \"fit\" not in d or d[\"fit\"] in (None, \"\", \"nan\"):\n",
    "            continue\n",
    "        X.append(\n",
    "            text_features(d, cat_words, cat_wordId, cat_wordSet, field=\"review_text\")\n",
    "        )\n",
    "        y.append(d[\"fit\"])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_fit_train, y_fit_train = build_fit_data(train_data)\n",
    "X_fit_valid, y_fit_valid = build_fit_data(valid_data)\n",
    "\n",
    "print(\"Fit prediction:\")\n",
    "print(\"  Train samples:\", X_fit_train.shape[0])\n",
    "print(\"  Valid samples:\", X_fit_valid.shape[0])\n",
    "\n",
    "fit_le = LabelEncoder()\n",
    "y_fit_train_enc = fit_le.fit_transform(y_fit_train)\n",
    "y_fit_valid_enc = fit_le.transform(y_fit_valid)\n",
    "\n",
    "fit_clf = LogisticRegression(max_iter=1000, multi_class=\"auto\")\n",
    "fit_clf.fit(X_fit_train, y_fit_train_enc)\n",
    "\n",
    "fit_train_acc = (fit_clf.predict(X_fit_train) == y_fit_train_enc).mean()\n",
    "fit_valid_acc = (fit_clf.predict(X_fit_valid) == y_fit_valid_enc).mean()\n",
    "\n",
    "print(\"  Train accuracy:\", fit_train_acc)\n",
    "print(\"  Valid accuracy:\", fit_valid_acc)\n",
    "\n",
    "\n",
    "def writePredictionsFit(\n",
    "    model, words, wordId, wordSet, label_encoder, in_pairs_path, out_path\n",
    "):\n",
    "    with open(out_path, \"w\") as predictions, open(in_pairs_path) as pairs:\n",
    "        for l in pairs:\n",
    "            if l.startswith(\"userID\"):\n",
    "                predictions.write(l)\n",
    "                continue\n",
    "            u, b = l.strip().split(\",\")\n",
    "            # Again, you'd need (u,b) → review_text mapping here.\n",
    "            feat = [0] * len(words) + [1]\n",
    "            pred_label = model.predict(np.array(feat).reshape(1, -1))[0]\n",
    "            pred_fit = label_encoder.inverse_transform([pred_label])[0]\n",
    "            predictions.write(u + \",\" + b + \",\" + str(pred_fit) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-uv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
